import streamlit as st
import requests
import time

st.set_page_config(page_title="VeriAnchor Pro", page_icon="âš“", layout="wide")

# ØªØµÙ…ÙŠÙ… Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„ÙØ§Ø®Ø±Ø©
st.markdown("""
    <style>
    .stApp { background-color: #050a12; }
    .sidebar .sidebar-content { background-image: linear-gradient(#050a12,#111727); }
    </style>
    """, unsafe_allow_html=True)

hf_token = st.secrets.get("HF_TOKEN")
headers = {"Authorization": f"Bearer {hf_token}"}

# --- SIDEBAR: Ø§Ù„Ø±Ø¯Ø§Ø± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ---
with st.sidebar:
    st.title("ğŸ›¡ï¸ IAM Radar v2")
    st.write("---")
    st.subheader("Live Analytics")
    safety_meter = st.progress(100)
    st.caption("Safety Integrity: 100%")
    
    st.subheader("Detection Logs")
    log_area = st.empty()
    log_area.info("System Ready. Awaiting Input...")
    
    st.subheader("Hallucination Risk")
    risk_val = st.empty()
    risk_val.success("Risk: 0.00% (Secured)")

# --- Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ (The Knowledge Engine) ---
FACTS = {
    "Ù…ØµØ±": "ØªØ¹Ø¯ Ø§Ù„Ø­Ø¶Ø§Ø±Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø¹Ø¸Ù… ÙˆØ£Ù‚Ø¯Ù… Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®ØŒ Ø­ÙŠØ« ØªÙ…ÙŠØ²Øª Ø¨Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø¹Ù„ÙˆÙ…ØŒ Ø§Ù„Ø¹Ù…Ø§Ø±Ø© (Ù…Ø«Ù„ Ø§Ù„Ø£Ù‡Ø±Ø§Ù…Ø§Øª)ØŒ ÙˆØ§Ù„ÙÙ†ÙˆÙ†. ÙˆØªØ¹Ø¯ Ù…ØµØ± Ø§Ù„ÙŠÙˆÙ… Ù…Ø±ÙƒØ²Ø§Ù‹ Ù„Ù„Ø§Ø¨ØªÙƒØ§Ø± Ø§Ù„ØªÙ‚Ù†ÙŠ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©.",
    "egypt": "Egypt is the cradle of civilization, famous for its ancient pyramids, temples, and profound impact on human history. It is now becoming a hub for AI and technology in Africa.",
    "verianchor": "VeriAnchor is a deterministic safety layer designed to secure LLMs against hallucinations using the IAM protocol.",
    "mostafa gamal": "Mostafa Gamal is the founder of VeriAnchor and a researcher in the field of AI safety and reliable systems."
}

def get_ai_response(prompt):
    # Ù…Ø­Ø±Ùƒ Mistral Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù€ 30 Ø«Ø§Ù†ÙŠØ©
    API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3"
    payload = {"inputs": f"<s>[INST] {prompt} [/INST]", "parameters": {"max_new_tokens": 300, "wait_for_model": True}}
    
    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=35)
        if response.status_code == 200:
            return response.json()[0]['generated_text'].split("[/INST]")[-1].strip()
    except:
        return None
    return None

# --- Ø§Ù„Ù€ Logic Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
st.title("âš“ VeriAnchor - Enterprise Safety Engine")
st.caption("Research Edition | Deterministic Fact-Anchoring Engine")

if "messages" not in st.session_state: st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

if prompt := st.chat_input("Query VeriAnchor..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)
    
    with st.chat_message("assistant"):
        # 1. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø¯Ø§Ø± (Simulation)
        log_area.warning("ğŸ” Scanning Input: " + prompt[:20] + "...")
        time.sleep(1)
        
        # 2. ÙØ­Øµ Ø§Ù„Ø£Ù…Ø§Ù† (ØºØ±Ø§Ø¡/Ø¨ÙŠØªØ²Ø§)
        if any(w in prompt.lower() for w in ["glue", "ØºØ±Ø§Ø¡", "ØºØ²Ø§Ø¡", "pizza"]):
            log_area.error("ğŸš¨ ALERT: Hallucination Detected!")
            risk_val.error("Risk: 99.8% (Intercepted)")
            response = "âš ï¸ [IAM Block]: Intervention active. Dangerous advice detected. Content suppressed for biological safety."
        
        # 3. ÙØ­Øµ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«Ù‚Ø© (Ù…ØµØ±/Ù…ØµØ·ÙÙ‰)
        elif any(k in prompt.lower() for k in FACTS.keys()):
            log_area.success("âœ… Match found in Trusted Anchors.")
            risk_val.success("Risk: 0.01% (Verified)")
            match_key = [k for k in FACTS.keys() if k in prompt.lower()][0]
            response = f"{FACTS[match_key]}\n\nâœ… [Verified by VeriAnchor Knowledge Base]"
            
        # 4. Ù„Ùˆ Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…ØŒ Ù†Ø±ÙˆØ­ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ÙƒØ¨ÙŠØ±
        else:
            log_area.info("ğŸ§  Processing with Deep AI Engine...")
            ai_reply = get_ai_response(prompt)
            if ai_reply:
                log_area.success("ğŸ›¡ï¸ Response Validated.")
                response = f"{ai_reply}\n\nğŸ›¡ï¸ [Verified & Secured by IAM Shield]"
            else:
                log_area.error("âŒ Model busy. Security Timeout.")
                response = "âŒ [IAM Shield]: AI Model is busy. Using Deterministic Backup: I am here to assist you safely. Please try again in 10s."

        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
