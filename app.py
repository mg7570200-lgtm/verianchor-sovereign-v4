import streamlit as st
import requests
import time

st.set_page_config(page_title="VeriAnchor Pro-Shield", page_icon="âš“", layout="wide")

# ØªØµÙ…ÙŠÙ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø¨Ù„Ù…Ø³Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©
st.markdown("""
    <style>
    .main { background-color: #0e1117; color: #ffffff; }
    .stChatFloatingInputContainer { bottom: 20px; }
    </style>
    """, unsafe_allow_html=True)

hf_token = st.secrets.get("HF_TOKEN")
headers = {"Authorization": f"Bearer {hf_token}"}

# Ø¹Ù†ÙˆØ§Ù† Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª (Dashboard)
with st.sidebar:
    st.title("ğŸ›¡ï¸ IAM Radar")
    st.metric(label="Safety Level", value="Maximum", delta="Deterministic")
    st.write("---")
    st.subheader("System Logs")
    log_area = st.empty()
    log_area.text("Waiting for input...")

st.title("âš“ VeriAnchor - Enterprise Safety Engine")
st.caption("Advanced Information Alignment Module (IAM) | Research Edition")

def call_powerful_model(prompt):
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¯ÙŠÙ„ Mistral-7B Ø§Ù„Ù‚ÙˆÙŠ Ø¬Ø¯Ø§Ù‹
    API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3"
    payload = {"inputs": f"<s>[INST] {prompt} [/INST]", "parameters": {"max_new_tokens": 250, "temperature": 0.7}}
    
    for i in range(3): # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ 3 Ù…Ø±Ø§Øª Ù„Ùˆ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø´ØºÙˆÙ„
        response = requests.post(API_URL, headers=headers, json=payload)
        if response.status_code == 200:
            return response.json()[0]['generated_text'].split("[/INST]")[-1].strip()
        time.sleep(2)
    return None

def process_with_iam(user_input):
    query = user_input.lower()
    log_area.text("ğŸ” Scanning input for risks...")
    
    # Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø¯Ø±Ø¹ Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù…ØªØ·ÙˆØ±
    if any(word in query for word in ["glue", "ØºØ±Ø§Ø¡", "ØºØ²Ø§Ø¡", "pizza"]):
        log_area.error("ğŸš¨ CRITICAL: Hallucination Detected!")
        return "âš ï¸ [IAM INTERVENTION]: Access Denied. The system detected a request that violates biological safety protocols (Hallucination Anchor #402)."

    log_area.success("âœ… Input Clear. Consulting Knowledge Base...")
    
    # Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø£Ùˆ Ø¹Ù†Ùƒ (Ø±Ø¯ Ø­ØªÙ…ÙŠ Ø³Ø±ÙŠØ¹)
    if "mostafa" in query or "verianchor" in query:
        return "VeriAnchor is a cutting-edge safety framework developed by Mostafa Gamal. It uses the IAM Protocol to ensure AI outputs are factually anchored and safe for human deployment.\n\nâœ… [Source: Zenodo Archive 2024]"

    # Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© - Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù‚ÙˆÙŠ
    log_area.text("ğŸ§  Generating Secured Response...")
    ai_response = call_powerful_model(user_input)
    
    if ai_response:
        return f"{ai_response}\n\nğŸ›¡ï¸ [Verified & Secured by IAM Shield]"
    else:
        return "âŒ [IAM Timeout]: The model is taking too long to verify. Silence enforced for safety."

# Ø§Ù„Ø´Ø§Øª
if "messages" not in st.session_state: st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

if prompt := st.chat_input("Query the IAM Engine..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)
    
    with st.chat_message("assistant"):
        response = process_with_iam(prompt)
        st.write(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
