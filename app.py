import streamlit as st
import requests
import time

st.set_page_config(page_title="VeriAnchor Pro", page_icon="âš“", layout="wide")

# ØªØµÙ…ÙŠÙ… Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„ÙØ§Ø®Ø±Ø©
st.markdown("<style>.stApp { background-color: #050a12; }</style>", unsafe_allow_html=True)

hf_token = st.secrets.get("HF_TOKEN")
headers = {"Authorization": f"Bearer {hf_token}"}

# --- SIDEBAR: Ø§Ù„Ø±Ø¯Ø§Ø± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ---
with st.sidebar:
    st.title("ğŸ›¡ï¸ IAM Radar v2")
    st.write("---")
    st.subheader("Live Analytics")
    safety_meter = st.progress(100)
    st.subheader("Detection Logs")
    log_area = st.empty()
    log_area.info("System Ready.")
    risk_val = st.empty()
    risk_val.success("Risk: 0.00% (Secured)")

# --- Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© (Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„ÙÙˆØ±ÙŠØ©) ---
FACTS = {
    "Ù…ØµØ±": "ØªØ¹Ø¯ Ø§Ù„Ø­Ø¶Ø§Ø±Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£Ø¹Ø¸Ù… ÙˆØ£Ù‚Ø¯Ù… Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®ØŒ ØªÙ…ÙŠØ²Øª Ø¨Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„Ø¹Ù…Ø§Ø±Ø©.",
    "Ø£Ø³ÙŠÙˆØ·": "Ø£Ø³ÙŠÙˆØ· Ù‡ÙŠ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø£ÙƒØ¨Ø± Ù…Ø­Ø§ÙØ¸Ø§Øª ØµØ¹ÙŠØ¯ Ù…ØµØ±ØŒ ÙˆØªØ¹ØªØ¨Ø± Ù…Ø±ÙƒØ²Ø§Ù‹ ØªØ¬Ø§Ø±ÙŠØ§Ù‹ ÙˆØªØ¹Ù„ÙŠÙ…ÙŠØ§Ù‹ Ù‡Ø§Ù…Ø§Ù‹ ÙˆØªØ¶Ù… Ø¬Ø§Ù…Ø¹Ø© Ø£Ø³ÙŠÙˆØ· Ø§Ù„Ø¹Ø±ÙŠÙ‚Ø©.",
    "verianchor": "VeriAnchor is a deterministic safety layer designed to secure LLMs using the IAM protocol.",
    "mostafa gamal": "Mostafa Gamal is the visionary founder of VeriAnchor and the developer of the IAM Protocol."
}

def call_model(model_id, prompt, timeout=15):
    url = f"https://api-inference.huggingface.co/models/{model_id}"
    try:
        response = requests.post(url, headers=headers, json={"inputs": prompt, "parameters": {"max_new_tokens": 150}}, timeout=timeout)
        if response.status_code == 200:
            return response.json()[0]['generated_text'].strip()
    except:
        return None
    return None

def get_smart_response(prompt):
    # 1. Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„Ø§Ù‚
    log_area.info("ğŸ§  Attempting Heavy Engine (Mistral)...")
    res = call_model("mistralai/Mistral-7B-Instruct-v0.3", prompt)
    if res: return res, "Heavy Engine"

    # 2. Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹ (Ø§Ù„Ø¨Ø¯ÙŠÙ„)
    log_area.warning("âš¡ Switching to High-Speed Engine (Qwen)...")
    res = call_model("Qwen/Qwen2.5-1.5B-Instruct", prompt)
    if res: return res, "Speed Engine"

    return None, None

# --- Ø§Ù„Ù€ Logic Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
st.title("âš“ VeriAnchor - Enterprise Safety Engine")
if "messages" not in st.session_state: st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

if prompt := st.chat_input("Query VeriAnchor..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)
    
    with st.chat_message("assistant"):
        log_area.warning("ğŸ” Scanning Input...")
        
        # 1. ÙØ­Øµ Ø§Ù„Ø£Ù…Ø§Ù†
        if any(w in prompt.lower() for w in ["glue", "ØºØ±Ø§Ø¡", "ØºØ²Ø§Ø¡"]):
            response = "âš ï¸ [IAM Block]: Intervention active. Dangerous advice suppressed."
            log_area.error("ğŸš¨ Hallucination Blocked!")
        
        # 2. ÙØ­Øµ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        elif any(k in prompt.lower() for k in FACTS.keys()):
            match_key = [k for k in FACTS.keys() if k in prompt.lower()][0]
            response = f"{FACTS[match_key]}\n\nâœ… [Verified by VeriAnchor Knowledge Base]"
            log_area.success("âœ… Match found in Anchors.")

        # 3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ© (Ù…Ø¹ Ø¨Ø¯ÙŠÙ„)
        else:
            ai_reply, engine_used = get_smart_response(prompt)
            if ai_reply:
                response = f"{ai_reply}\n\nğŸ›¡ï¸ [Verified & Secured via {engine_used}]"
                log_area.success(f"ğŸ›¡ï¸ Response via {engine_used}")
            else:
                response = "âŒ [IAM Shield]: All engines busy. Deterministic safety active. Please retry in 5s."
                log_area.error("âŒ Critical Timeout.")

        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
